{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://onnxruntime.ai/docs/execution-providers/CUDA-ExecutionProvider.html#requirementsto\n",
    "# https://developer.nvidia.com/rdp/cudnn-archive\n",
    "# https://medium.com/@Rahul_Meduri/install-cuda-cudnn-in-conda-virtual-environment-and-setup-gpu-support-using-tensorflow-f8a4c942b6ea\n",
    "\n",
    "# https://huggingface.co/docs/optimum/main/en/exporters/onnx/usage_guides/export_a_model#custom-export-of-transformers-models\n",
    "# https://huggingface.co/google/siglip-base-patch16-224/tree/main\n",
    "# https://huggingface.co/docs/transformers/main/model_doc/siglip#transformers.SiglipTextModel\n",
    "# https://huggingface.co/blog/convert-transformers-to-onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoTokenizer,\n",
    "    SiglipModel,\n",
    "    SiglipVisionModel,\n",
    "    SiglipTextModel,\n",
    ")\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_key = os.getenv(\"HF_KEY\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the image from the url\n",
    "headers = {\"Authorization\": f\"Bearer {hf_key}\"}\n",
    "API_URL = \"https://datasets-server.huggingface.co/rows?dataset=huggan/wikiart&config=default&split=train&offset=1&length=100\"\n",
    "\n",
    "\n",
    "def query():\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "data = query()\n",
    "len(data[\"rows\"])  # 100 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lib/storage/miniconda3/envs/hcmai/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load the model and the tokenizer\n",
    "model_org = SiglipModel.from_pretrained(\"nielsr/siglip-base-patch16-224\").to(device)\n",
    "processor = AutoProcessor.from_pretrained(\"nielsr/siglip-base-patch16-224\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nielsr/siglip-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siglip Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = data[\"rows\"][0][\"row\"][\"image\"][\"src\"]\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "vision_model = SiglipVisionModel.from_pretrained(\"nielsr/siglip-base-patch16-224\")\n",
    "torch_out = vision_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lib/storage/miniconda3/envs/hcmai/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:353: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n",
      "/var/lib/storage/miniconda3/envs/hcmai/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:371: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    vision_model,\n",
    "    tuple(inputs.values())[0],\n",
    "    f=\"siglip_vision.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"siglip_vision.onnx\",\n",
    "    providers=[\n",
    "        \"CUDAExecutionProvider\",\n",
    "        \"CPUExecutionProvider\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return (\n",
    "        tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference of Pytorch model used 0.021751880645751953 seconds\n",
      "Inference of ONNX model used 0.03810739517211914 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "with torch.no_grad():\n",
    "    url = data[\"rows\"][0][\"row\"][\"image\"][\"src\"]\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    start = time.time()\n",
    "    torch_out = model_org.get_image_features(**inputs.to(device))\n",
    "    end = time.time()\n",
    "    print(f\"Inference of Pytorch model used {end - start} seconds\")\n",
    "\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(tuple(inputs.values())[0])}\n",
    "    start = time.time()\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    end = time.time()\n",
    "    print(f\"Inference of ONNX model used {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time of inference onnx model is higher than pytorch model on GPU a bit\n",
    "# but on CPU it is faster\n",
    "# i don't know why :v\n",
    "# maybe torch 2.0 optimize the model better than onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1876378e-09"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ort_outs[1] - torch_out.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/hieugn/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "siglip_vision.onnx: 100%|██████████| 372M/372M [00:41<00:00, 8.89MB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hieuGoku/siglip_onnx/commit/3d4e92c95a038ac1b5c03e24af0cfff2b2641865', commit_message='Upload siglip_vision.onnx with huggingface_hub', commit_description='', oid='3d4e92c95a038ac1b5c03e24af0cfff2b2641865', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "login(hf_key)\n",
    "\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"siglip_vision.onnx\",\n",
    "    path_in_repo=\"siglip_vision.onnx\",\n",
    "    repo_id=\"hieuGoku/siglip_onnx\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siglip Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a woman\"\n",
    "text_token = tokenizer([prompt], return_tensors=\"pt\")\n",
    "text_model = SiglipTextModel.from_pretrained(\"nielsr/siglip-base-patch16-224\")\n",
    "torch_out = text_model(**text_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/lib/storage/miniconda3/envs/hcmai/lib/python3.10/site-packages/torch/onnx/utils.py:2094: UserWarning: Provided key attention_mask for dynamic axes is not a valid input/output name\n",
      "  warnings.warn(\n",
      "/var/lib/storage/miniconda3/envs/hcmai/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:353: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n",
      "/var/lib/storage/miniconda3/envs/hcmai/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py:371: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    text_model,\n",
    "    tuple(text_token.values())[0],\n",
    "    f=\"siglip_text.onnx\",\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "        \"output\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "    },\n",
    "    do_constant_folding=True,\n",
    "    opset_version=13,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_session = onnxruntime.InferenceSession(\n",
    "    \"siglip_text.onnx\", providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
    ")\n",
    "\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return (\n",
    "        tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference of Pytorch model used 0.023141860961914062 seconds\n",
      "Inference of ONNX model used 0.0066034793853759766 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"a woman\"\n",
    "text_token = tokenizer([prompt], return_tensors=\"pt\")\n",
    "\n",
    "start = time.time()\n",
    "text_features = model_org.get_text_features(**text_token.to(device))\n",
    "end = time.time()\n",
    "print(f\"Inference of Pytorch model used {end - start} seconds\")\n",
    "\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(tuple(text_token.values())[0])}\n",
    "start = time.time()\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "end = time.time()\n",
    "print(f\"Inference of ONNX model used {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.3614924e-09"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ort_outs[1] - text_features.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "siglip_text.onnx: 100%|██████████| 441M/441M [00:41<00:00, 10.6MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hieuGoku/siglip_onnx/commit/e1e7da6d2f08927e4ad6bd3d3c133e3ecd40b258', commit_message='Upload siglip_text.onnx with huggingface_hub', commit_description='', oid='e1e7da6d2f08927e4ad6bd3d3c133e3ecd40b258', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.upload_file(\n",
    "    path_or_fileobj=\"siglip_text.onnx\",\n",
    "    path_in_repo=\"siglip_text.onnx\",\n",
    "    repo_id=\"hieuGoku/siglip_onnx\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
